{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3988f315",
   "metadata": {},
   "source": [
    "# Deep Learning - MÃ¡ster in Artificial Intelligence (UDC)\n",
    "## Amazon Reviews Sentiment Classification using RNNs - \n",
    "Date: 20/03/2025\n",
    "\n",
    "Authors:\n",
    "\n",
    "Paula Biderman Mato\n",
    "\n",
    "Celia Hermoso Soto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2644306",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this part of the practice, the main objective is to build and evaluate different Recurrent Neural Networks (RNNs) for text classification. The dataset used consists of Amazon customer reviews, labeled with positive or negative sentiment. The goal is to train deep learning models that can accurately predict whether a review is positive (4 or 5 stars) or negative (1 or 2 stars).\n",
    "\n",
    "As we have seen throughout this subject, RNNs are a powerful type of deep learning architecture especially suited for sequential data such as text. They are able to capture temporal dependencies by maintaining a hidden state that is passed through time. In this practice, we use three RNN-based architectures: a simple RNN, an LSTM, and a bidirectional LSTM. These models are trained on a preprocessed version of the Amazon reviews dataset using Keras.\n",
    "\n",
    "Unlike CNNs in image classification, text classification problems require models that understand the sequential nature of language. RNNs, and in particular LSTMs, are designed to capture such dependencies, which makes them an appropriate choice for this task. In this notebook, we compare the performance of each architecture in terms of classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556a9ab",
   "metadata": {},
   "source": [
    "To reuse functions and code outside a Jupyter environment, the notebook can be converted into a Python script using the nbconvert tool. This is particularly useful when we want to organize reusable components, such as data preprocessing or model definitions, into importable modules. The conversion is done with a single command: *!jupyter nbconvert --to script generateAmazonDataset.ipynb*\n",
    "This command transforms the notebook into a .py file by extracting all code cells and converting markdown cells into Python comments. The resulting script can then be edited, imported into other projects, or executed directly as a standalone Python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bec89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script generateAmazonDataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee71926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from generateAmazonDataset import readData, transformData\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1923fd",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "We use the script provided by the instructors to load the dataset: generateAmazonDataset.ipynb. It contains two functions: readData and transformData. The readData function returns the training and test sets in raw text format, along with their binary sentiment labels. __label__1 corresponds to negative reviews (1 or 2 stars) and __label__2 to positive reviews (4 or 5 stars). Neutral reviews (3 stars) have been excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76411720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts, test_labels = readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb34818",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We print the number of samples in each set to confirm the data has been correctly loaded.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# We print the number of samples in each set to confirm the data has been correctly loaded.\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abeef5",
   "metadata": {},
   "source": [
    "# Transform data using TextVectorization\n",
    "In order to use text as input for a neural network, we must convert it into numeric format. We use the Keras layer TextVectorization, which converts raw text into sequences of integers, where each integer represents a word in a vocabulary limited to 'max_features'.\n",
    "* max_features: the number of most frequent words to keep in the vocabulary.\n",
    "* seq_length: the maximum number of words per input review. Shorter reviews are padded with zeros.\n",
    "* embedding_dim: the size of the dense vector that each token will be mapped to in the Embedding layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "seq_length = 300\n",
    "embedding_dim = 64\n",
    "\n",
    "X_train, y_train, X_test, y_test, vectorizer = transformData(\n",
    "    train_texts, train_labels, test_texts, test_labels,\n",
    "    max_features=max_features,\n",
    "    output_sequence_length=seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9810576",
   "metadata": {},
   "source": [
    "After transformation, each input review is a sequence of integers of fixed length. These inputs are now ready to be used in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d2d7e",
   "metadata": {},
   "source": [
    "# Build Model 1: SimpleRNN\n",
    "The first model uses a SimpleRNN layer. This is the most basic type of recurrent layer. It maintains a hidden state that gets updated sequentially, allowing the model to capture dependencies in the text. However, it tends to struggle with long-term dependencies, which will be improved in later models using LSTM.\n",
    "Architecture:\n",
    "* Embedding layer: transforms integer word indices into dense vectors of fixed size (embedding_dim)\n",
    "* SimpleRNN: processes the sequence one word at a time and outputs a hidden state\n",
    "* Dense: final classification layer with sigmoid activation (since this is a binary classification task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.Sequential([\n",
    "    layers.Embedding(max_features, embedding_dim, input_length=seq_length),\n",
    "    layers.SimpleRNN(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_rnn = model_rnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cfae4",
   "metadata": {},
   "source": [
    "# Build Model 2: LSTM + Dropout\n",
    "This model uses an LSTM layer, which is a more advanced type of RNN capable of learning long-term dependencies. We add a Dropout layer after LSTM to reduce overfitting and improve generalization.\n",
    "Architecture:\n",
    "* Embedding layer\n",
    "* LSTM: more robust to vanishing gradients, ideal for longer sequences\n",
    "* Dropout: randomly disables 50% of neurons during training\n",
    "* Dense: output layer for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac494941",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.Sequential([\n",
    "    layers.Embedding(max_features, embedding_dim, input_length=seq_length),\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_lstm = model_lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d64cc",
   "metadata": {},
   "source": [
    "# Build Model 3: Bidirectional LSTM\n",
    "In this model, we wrap the LSTM layer in a Bidirectional wrapper. This allows the model to read the input sequence both forward and backward, capturing context from both directions, which is useful for understanding sentence structure.\n",
    "Architecture:\n",
    "* Embedding layer\n",
    "* Bidirectional LSTM: processes the sequence in both directions\n",
    "* Dense: final output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fad784",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bilstm = keras.Sequential([\n",
    "    layers.Embedding(max_features, embedding_dim, input_length=seq_length),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_bilstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_bilstm = model_bilstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2cf02",
   "metadata": {},
   "source": [
    "# Visualization of training results\n",
    "We plot the accuracy of each model over the training epochs to visually compare their performance. This helps to detect underfitting, overfitting, and general learning trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title):\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='val')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_rnn, 'SimpleRNN Accuracy')\n",
    "plot_history(history_lstm, 'LSTM Accuracy')\n",
    "plot_history(history_bilstm, 'Bidirectional LSTM Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fce50",
   "metadata": {},
   "source": [
    "# Final evaluation and conclusions\n",
    "In this section we show the final results on the test set. The main metric used is accuracy. We compare the performance of each architecture to assess which one generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0e847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Evaluation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleRNN:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel_rnn\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_lstm\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBidirectional LSTM:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_bilstm\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Final Evaluation:\")\n",
    "print(\"SimpleRNN:\", model_rnn.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"LSTM:\", model_lstm.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"Bidirectional LSTM:\", model_bilstm.evaluate(X_test, y_test, verbose=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
